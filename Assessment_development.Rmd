---
title: "Assessment_development_using_ITR"
output: html_document
date: "2025-09-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
pkgs <- c('mirt','catR','dplyr','readr','ggplot2','psych','lavaan','lordif','data.table','itembankr','gridExtra')
install_if_missing <- function(p) if(!requireNamespace(p, quietly = TRUE)){install.packages(p)}
invisible(lapply(pkgs, install_if_missing))
library(readr); library(psych); library(lavaan); library(dplyr);library(mirt); library(ggplot2); library(catR);
```

```{r}
resp <- read_csv('pilot_responses.csv') %>% select(starts_with('Q'))
```




# Cronbach alpha and omega
```{r}
alpha_val <- psych::alpha(resp)
print(alpha_val$total)
```
```{r}
omega_val <- psych::omega(resp, nfactors=3, plot=FALSE)
print(omega_val)
```


# Parallel analysis to suggest number of factors
```{r}
fa.parallel(resp, fa='fa')
```


# Simple EFA
```{r}
efa <- fa(resp, nfactors=3, fm='ml', rotate='oblimin')
print(efa)
```



# If you have a hypothesized one-factor model, run CFA
```{r}
model1 <- 'F1 =~ '
model1 <- paste0("F1 =~ ", paste(names(resp), collapse=' + '))
fit1 <- cfa(model1, data = resp)
summary(fit1, fit.measures=TRUE)
```
# Decision: if evidence supports unidimensionality -> proceed with unidim IRT, else consider MIRT
```

**Notes:**  
  - High alpha alone does not prove unidimensionality — use EFA/CFA and parallel analysis.  
- If more than one factor emerges, either split test by dimension or use multidimensional IRT (via `mirt` multidimensional models).

---
  
  ## 3) Calibrate items (`scripts/03_calibrate_items.R`)
  - Fit 2PL and 3PL models, compare fit, and save estimated parameters.

```r
# 03_calibrate_items.R
```{r}
library(mirt); library(readr); library(dplyr)
resp_df <- read_csv('pilot_responses.csv')
resp_mat <- resp_df %>% select(starts_with('Q'))
```

# Fit 2PL first (more stable). If you have large N and want guessing, try 3PL.
```{r}
mod_3pl <- mirt(resp_mat, 1, itemtype='3PL', technical=list(NCYCLES=4000))
```
# Compare AIC/BIC
```{r}
cat('3PL AIC/BIC:', AIC(mod_3pl), BIC(mod_3pl), '\n')
```



# Extract parameters (IRT parameterization: a, b, g)
```{r}
pars3 <- coef(mod_3pl, IRTpars=TRUE, simplify=TRUE)$items
print(pars3)
```
```{r}
write_csv(as.data.frame(pars3) %>% tibble::rownames_to_column('item'), 'item_parameters_3pl.csv')
```

```{r}
saveRDS(mod_3pl, 'model_3pl.rds')
```


**Explanation:**  
  - Start with 2PL if sample size is moderate; 3PL requires larger samples for stable c estimates.  
- Use information criteria and item fit to choose model.

---
  
  ## 4) Item analysis & Information (`scripts/04_item_analysis_and_information.R`)
  - Generate ICCs, IIFs, TIF, item fit statistics, flag items with low discrimination or poor fit, and run DIF (lordif) for a grouping variable if available.

```r
# 04_item_analysis_and_information.R
library(mirt); library(ggplot2); library(readr); library(dplyr)
```{r}
mod <- readRDS('model_3pl.rds')
```

```{r}
resp_df <- read_csv('pilot_responses.csv')
```

```{r}
pars <- read_csv('item_parameters_3pl.csv')
```


# 1) Plot ICCs for all the 40 items
```{r}
pdf('outputs/plots/item_ICCs.pdf', width=8, height=10)
plot(mod, type='trace', which.items=1:40)
dev.off()
```


# 2) Item information traces
```{r}
pdf('outputs/plots/item_info_traces.pdf', width=8, height=10)
plot(mod, type='infotrace', which.items=1:12)
dev.off()
```



# 3) Test Information Function
```{r}
pdf('outputs/plots/test_information.pdf', width=8, height=4)
plot(mod, type='info')
dev.off()
```




# 4) Compute item fit
```{r}
item_fit_stats <- itemfit(mod)
write_csv(as.data.frame(item_fit_stats), 'outputs/item_fit_stats.csv')
```



# 5) Flag items: a < 0.5 or poor fit (p < 0.01)
```{r}
item_params <- as.data.frame(pars) %>% rename(item = item)
flagged <- item_params %>% filter(a1 < 0.5 | `p.S_X2` < 0.01)
write_csv(flagged, 'outputs/flagged_items.csv')
```



# 6) DIF example - requires a grouping vector; here we simulate a group split for demo
```{r}
set.seed(1)
group <- sample(c('A','B'), nrow(resp_df), replace=TRUE)
```


# lordif usage simplified; real DIF needs proper subgrouping
# library(lordif)
# difres <- lordif(resp_df %>% select(starts_with('Q')), group)

cat('Item analysis complete. Check outputs/plots and outputs/\n')
```

**Important:**  
  - Item Information Functions (IIF) show where each item provides precision across θ.  
- The Test Information Function (TIF) tells you overall where the test is most precise — target your intended population.
  
  ## 5) Assemble final bank with content balancing & exposure control
```{r}
(`scripts/05_assemble_final_bank.R`)
```


  - Remove flagged items, ensure coverage across difficulty, add content tags, and set initial exposure control parameters.


# 05_assemble_final_bank.R
```{r}
library(readr); library(dplyr)
pars <- read_csv('outputs/item_parameters_3pl.csv')
flagged <- read_csv('outputs/flagged_items.csv')
```

# Remove flagged items
```{r}
if(nrow(flagged)>0) pars <- pars %>% filter(!item %in% flagged$item)
```



# Add metadata (for demo we'll assign content tags cycling)
```{r}
pars <- pars %>% mutate(content = rep(c('Arithmetic','Algebra','Geometry'), length.out=nrow(pars)), exposure = 0, in_bank = TRUE)
```



# Ensure difficulty coverage: create bins on b and pick from each bin

```{r}
pars <- pars %>% arrange(b)
pars$bin <- ntile(pars$b, 10)
selected <- pars %>% group_by(bin) %>% slice_head(n=1) %>% ungroup()
```


# Aim for final size 30

```{r}
target <- 30
if(nrow(selected) < target){ extras <- pars %>% filter(!item %in% selected$item) %>% slice_head(n = target - nrow(selected)); selected <- bind_rows(selected, extras)}
if(nrow(selected) > target) selected <- selected %>% slice_sample(n=target)

write_csv(selected, 'outputs/final_item_bank.csv')
cat('Final item bank saved at outputs/final_item_bank.csv\n')
```

**Exposure control note:**  
  - In production, implement Sympson–Hetter or set maximum exposure rates and use top-k randomization for selection among high-information items.

  
  ## 6) CAT simulation & deployment (`scripts/06_cat_simulation_and_deployment.R`)
  - Demonstrates CAT admin with: starting rule, item selection by Maximum Fisher Information (MFI), top-k randomization (exposure control), content constraints (simple form), ability updating (EAP/MAP/MLE), and stopping rules (SE threshold or max items).


# 06_cat_simulation_and_deployment.R

```{r}
library(catR); library(readr); library(dplyr)
bank <- read_csv('outputs/final_item_bank.csv')
bank_mat <- as.matrix(bank %>% select(a1, b, g)) # adjust column names if needed
colnames(bank_mat) <- c('a','b','c')

true_thetas <- read_csv('data/pilot_theta.csv') %>% slice_sample(n=100)
```

```{r}
results <- list()
for(i in seq_len(nrow(true_thetas))){
  true_theta <- true_thetas$true_theta[i]
  administered <- c(); responses <- c()
  theta_hat <- 0; se <- 999
  max_items <- 12; target_se <- 0.33
  
  repeat{
    # get top candidate by Fisher information (MFI)
    ni <- nextItem(bank_mat, theta=theta_hat, out=administered, criterion='MFI')
    # top-k randomization example: get top 3 informative items and pick randomly among them
    info_vals <- iteminfo(bank_mat, theta=theta_hat)
    avail <- setdiff(seq_len(nrow(bank_mat)), administered)
    topk <- head(order(info_vals[avail], decreasing=TRUE), 3)
    chosen_idx <- sample(avail[topk], 1)
    
    # simulate response
    p <- bank_mat[chosen_idx,'c'] + (1 - bank_mat[chosen_idx,'c'])/(1 + exp(-bank_mat[chosen_idx,'a']*(true_theta - bank_mat[chosen_idx,'b'])))
    r <- rbinom(1,1,p)
    
    administered <- c(administered, chosen_idx)
    responses <- c(responses, r)
    
    # update ability (EAP as stable option)
    theta_hat <- thetaEst(bank_mat, resp.pattern=responses, items=administered, method='EAP')
    se <- semTheta(theta_hat, bank_mat, responses, administered, method='EAP')
    
    # exposure counter update (in real system, update DB counter)
    bank$exposure[chosen_idx] <- bank$exposure[chosen_idx] + 1
    
    if(se <= target_se || length(administered) >= max_items) break
  }
  results[[i]] <- data.frame(id=true_thetas$id[i], true_theta=true_theta, est_theta=theta_hat, se=se, n_items=length(administered))
}
final <- bind_rows(results)
write_csv(final, 'outputs/cat_simulation_results.csv')
cat('CAT simulation done. results in outputs/cat_simulation_results.csv\n')
```

**What to check after simulation:**  
  - Distribution of items used (exposure rates).  
- Accuracy: correlation between true_theta and est_theta, bias, RMSE.  
- Average number of items used vs fixed test of same precision.
  
  ## 7) Scoring examples: MLE, MAP, EAP (`scripts/07_scoring_examples_MLE_MAP_EAP.R`)
  - Show differences between MLE (no prior), MAP (with normal prior), and EAP (posterior mean) and compute SE for each.

```{r}
# 07_scoring_examples_MLE_MAP_EAP.R
library(mirt); library(readr)
mod <- readRDS('outputs/model_3pl.rds')
# pick one examinee's responses
resp <- read_csv('data/pilot_responses.csv')
one_resp <- as.numeric(resp[1, grepl('Q', names(resp))])

# MLE using mirt's fscores with method 'ML'
f_mle <- fscores(mod, method='ML', response.pattern=one_resp)
# MAP
f_map <- fscores(mod, method='MAP', response.pattern=one_resp)
# EAP
f_eap <- fscores(mod, method='EAP', response.pattern=one_resp)

cat('MLE:', f_mle, '\nMAP:', f_map, '\nEAP:', f_eap, '\n')
```

# For CAT we used thetaEst from catR for EAP; semTheta provides SE


**Notes on scoring:**  
  - **MLE**: unbiased for large tests but undefined when responses are all correct/incorrect.  
- **MAP**: adds prior (often N(0,1)) — stabilizes early-stage estimates.  
- **EAP**: posterior mean, uses full posterior distribution (robust), often used in CAT.


